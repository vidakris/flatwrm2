{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatw'rm2 training\n",
    "\n",
    "Use this notebook, if you want to retrain the flatw'rm2 network for any reason, e.g. you have a training set of light curves with a cadence that doesn't work well with the current weight file. Make sure to include a large number of flagged light curves along with some non-flaring targets. \n",
    "\n",
    "You can either start the training from scratch, or use transfer learning to reduce training time by loading the best current weight file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "import datetime\n",
    "from time import localtime, strftime\n",
    "\n",
    "from scipy.signal import medfilt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have multiple GPUs in your machine, you can change this to your preferred unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be used for feeding the input to the neural network during the training. As we are working with time-domain data and recurrent networks, the order, in which the data is fed to the network is not arbitrary. Also, batch number shouldn't be increased too high, the batch lenghts should be longer than the typical timescale of the flares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitGenerator(Sequence):\n",
    "    #Because... python.\n",
    "    def proper_round(self, val):\n",
    "        if (float(val) % 1) >= 0.5:\n",
    "            x = np.ceil(val)\n",
    "        else:\n",
    "            x = round(val)\n",
    "        return x\n",
    "    \n",
    "    def __init__(self, x_set, y_set, batch_size=1, length=10):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.length = length\n",
    "        if self.batch_size * self.length > len(self.y):\n",
    "            raise ValueError(\"Batch size and sample size don't match! batch_size=\",self.batch_size,\"length=\",self.length)\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = np.zeros( (self.batch_size, self.length) )\n",
    "        batch_y = np.zeros( self.batch_size )\n",
    "\n",
    "        if idx > self.__len__():\n",
    "            raise ValueError(\"Requested index too large\")\n",
    "\n",
    "        for b in np.arange(self.batch_size):\n",
    "            try:\n",
    "                batch_x[b] = self.x[ b * (len(self.y) // self.batch_size) + idx  : b * (len(self.y)//self.batch_size) + idx + self.length ].reshape(self.length)\n",
    "                batch_y[b] = self.proper_round ( np.mean ( self.y[ b * (len(self.y) // self.batch_size) + idx  : b * (len(self.y)//self.batch_size) + idx + self.length ] ) )\n",
    "            except:\n",
    "                batch_x_temp = self.x[ b * (len(self.y) // self.batch_size) + idx  : b * (len(self.y)//self.batch_size) + idx + self.length ]\n",
    "                endpoints  = self.x[ -self.length+len(batch_x_temp) : ][::-1]\n",
    "                batch_x[b] = np.concatenate((batch_x_temp,endpoints)).reshape(self.length)\n",
    "                \n",
    "                batch_y_temp = self.y[ b * (len(self.y) // self.batch_size) + idx  : b * (len(self.y)//self.batch_size) + idx + self.length ]\n",
    "                endpoints  = self.y[ -self.length+len(batch_y_temp) : ][::-1]\n",
    "                batch_y_temp = np.concatenate((batch_y_temp,endpoints)).reshape(self.length)\n",
    "                batch_y[b] = self.proper_round ( np.mean ( batch_y_temp ) )\n",
    "                \n",
    "                del endpoints,batch_x_temp,batch_y_temp\n",
    "                \n",
    "        return batch_x.reshape(self.batch_size, self.length,1) , batch_y.reshape( self.batch_size, 1  )\n",
    "    def __len__(self):\n",
    "        return int( np.ceil( len(self.y) / self.batch_size ) ) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to read the data files. If your cadence is not 1 minute, change it here, in case of long-cadence data, you might want to consider changing the length of the gaps, where the gaps get interpolated and filled up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, cadence=1./60/24):\n",
    "    from scipy.interpolate import interp1d\n",
    "    from scipy.stats import median_absolute_deviation\n",
    "    \n",
    "    from numpy.polynomial.polynomial import Polynomial\n",
    "    \n",
    "    time, flux, is_flare = np.genfromtxt(filename).T\n",
    "    \n",
    "\n",
    "\n",
    "    allgaps = np.diff(time) > 10./60/24\n",
    "    allgaps = np.where(allgaps>0)[0]\n",
    "\n",
    "\n",
    "    for ii in range(len(allgaps),0,-1):  \n",
    "        ii-=1\n",
    "        begin=time[allgaps[ii]]\n",
    "        end=time[allgaps[ii]+1]\n",
    "        gaplength=end-begin\n",
    "        to_check_std = 0.015 #day\n",
    "\n",
    "        um1 = (time>=time[allgaps[ii]]-to_check_std) &  (time<=time[allgaps[ii]])\n",
    "        um2 = (time>=time[allgaps[ii]+1]) &  (time<=time[allgaps[ii]+1]+to_check_std)\n",
    "        umplot = (time>=time[allgaps[ii]+1]-5*gaplength) &  (time<=time[allgaps[ii]+1]+5*gaplength)\n",
    "\n",
    "        down,up = np.percentile(flux[um1],[5,95])\n",
    "        beginstd = np.std( flux[um1][ (flux[um1]>=down) & (flux[um1]<=up) ] )\n",
    "        down,up = np.percentile(flux[um2],[5,95])\n",
    "        endstd = np.std( flux[um2][ (flux[um2]>=down) & (flux[um2]<=up) ] )\n",
    "        beginmean = np.mean( flux[allgaps[ii]-5:allgaps[ii]+1] )\n",
    "        endmean = np.mean( flux[allgaps[ii]+1:allgaps[ii]+5] )\n",
    "        npoints = int(gaplength/np.mean(np.diff(time)))\n",
    "\n",
    "        if beginstd<endstd:\n",
    "            filling = np.random.normal(loc=0,scale=beginstd,size=npoints)\n",
    "        else:\n",
    "            filling = np.random.normal(loc=0,scale=endstd,size=npoints)\n",
    "\n",
    "        timefilling  =np.linspace(begin,end,npoints+2)[1:-1]\n",
    "\n",
    "        time=np.insert(time, allgaps[ii]+1, timefilling)\n",
    "        flux=np.insert(flux, allgaps[ii]+1, filling+np.linspace(beginmean,endmean,npoints) )\n",
    "        is_flare=np.insert(is_flare, allgaps[ii]+1, np.zeros_like(timefilling) )\n",
    "        #plt.figure(figsize=(10,3))\n",
    "        #plt.scatter(lc[umplot,0],lc[umplot,1],s=5,c='k')\n",
    "        #plt.plot(lc[um1,0],lc[um1,1],c='C0')\n",
    "        #plt.plot(lc[um2,0],lc[um2,1],c='C0')\n",
    "\n",
    "        #plt.scatter(timefilling,filling+np.linspace(beginmean,endmean,npoints),c='r',s=3)\n",
    "        #plt.plot(timefilling,filling+np.linspace(beginmean,endmean,npoints),'r')\n",
    "        #plt.axvspan(lc[allgaps[ii],0],lc[allgaps[ii]+1,0],color='g',alpha=0.1,zorder=0)\n",
    "\n",
    "        #plt.show()\n",
    "\n",
    "    t = np.arange(time[0], time[-1], cadence)\n",
    "    bad_points = np.isnan(flux)\n",
    "\n",
    "    iflux = interp1d( time[~bad_points], \n",
    "                     flux[~bad_points]  ,\n",
    "                     fill_value=\"extrapolate\", \n",
    "                     bounds_error=False)\n",
    "    fl = iflux(t)\n",
    "\n",
    "    umcut = fl < np.percentile(fl,99.9)\n",
    "    fl/=np.mean(fl[umcut])\n",
    "    fl=(fl-1.) / np.ptp(fl[umcut])\n",
    "    \n",
    "    iflag = interp1d( time[~bad_points], is_flare[~bad_points], fill_value=\"extrapolate\", bounds_error=False)\n",
    "    filtflag = medfilt(  medfilt(iflag(t), 3), 9  )\n",
    "    return( np.array( [t, fl, np.round( filtflag )  ] ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure to check this block!\n",
    "\n",
    "The train set is defined in `train.txt`. If you want to use only a part of the files listed here for whatever reason, use the `filenumber` option. \n",
    "\n",
    "If you want to do a K-fold test, change the value of `split` to 0.8 for K=5, for example. In this case, also modify `np.arange(1)` to `np.arange(5)` in the next block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "def LoadLC(filenumber=900, split=0.99, kfold_n=0, trv_test_split=0.9):\n",
    "    from math import floor, ceil\n",
    "    from numpy import genfromtxt, nanmedian\n",
    "  \n",
    "    train_data = []\n",
    "    validation_data = []\n",
    "    time=flux=flag=np.array([])\n",
    "    vtime=vflux=vflag=np.array([])\n",
    "\n",
    "\n",
    "    with open (\"train.txt\", \"r\") as trainfile:\n",
    "        trainlist = trainfile.read().splitlines()\n",
    "\n",
    "    train_files = trainlist\n",
    "    validation_files = trainlist[-5:]            \n",
    "\n",
    "    print(\"Train: \")\n",
    "    for i in train_files:\n",
    "        print(i)\n",
    "\n",
    "        t, f, fl = read_data(i)\n",
    "        time = np.concatenate((time,t))\n",
    "        flux = np.concatenate((flux,f))\n",
    "        flag = np.concatenate((flag,fl))\n",
    "\n",
    "\n",
    "    print(\"Validation: \")\n",
    "    for i in validation_files:\n",
    "        print(i)\n",
    "\n",
    "        t, f, fl = read_data(i)\n",
    "        vtime = np.concatenate((vtime,t))\n",
    "        vflux = np.concatenate((vflux,f))\n",
    "        vflag = np.concatenate((vflag,fl))\n",
    "\n",
    "\n",
    "    train_data = np.array([time,flux,flag])\n",
    "    validation_data = np.array([vtime,vflux,vflag])\n",
    "\n",
    "    return train_data, validation_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will start the training from scratch with a three-layered LSTM(128) network. For transfer learning, comment out the model definition, and uncomment the `load_model` line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kfold_ni in np.arange(1):\n",
    "\n",
    "    train_data, validation_data = LoadLC(kfold_n=kfold_ni)\n",
    "  \n",
    "    X_train, y_train = np.array([train_data[1]-1+1]).T, np.array(train_data[2]).reshape(1, train_data[2].size)\n",
    "\n",
    "    X_test, y_test = np.array([validation_data[1]-1+1]).T, np.array(validation_data[2]).reshape(1, validation_data[2].size)\n",
    "    \n",
    "    \n",
    "\n",
    "    window_size = 64\n",
    "\n",
    "    batch=1024\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    generator = SplitGenerator(X_train, y_train.T, length=window_size, batch_size=batch)\n",
    "\n",
    "    generator_val = SplitGenerator(X_test, y_test.T, length=window_size, batch_size=batch)\n",
    "\n",
    "    units=128\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add( (LSTM(units, return_sequences=True)) )\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add( (LSTM(units, return_sequences=True)) )\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add( (LSTM(units)) )\n",
    "    model.add( Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    #model=tf.keras.models.load_model('./LSTM-fold_all-mixedtrain0.h5')\n",
    "\n",
    "    recall_metric = tf.keras.metrics.Recall()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy', recall_metric ])\n",
    "\n",
    "    tensorboard_callback = TensorBoard(log_dir='./logs/LSTM-02-fold_all_mixedtrain'+str(kfold_ni)+str(units)+strftime(\"%Y-%m-%d.%H:%M:%S\", localtime()), histogram_freq=1)\n",
    "    earlystop_callback = EarlyStopping(monitor='loss', mode='min', min_delta=1e-2, patience=10, restore_best_weights=True)\n",
    "    checkpoint_file = 'checkpoints/'+strftime(\"%Y-%m-%d.%H:%M:%S\", localtime())+'run_lstm_02-fold_all_mixedtrain'+str(kfold_ni)+'-{epoch:02d}-{accuracy:.2f}.h5'\n",
    "    checkpoint_callback = ModelCheckpoint(checkpoint_file, save_best_only=True,\n",
    "                                         verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "    flare_weight = y_train.size/y_train.sum()\n",
    "\n",
    "    history = model.fit(generator, \n",
    "                        epochs=100,\n",
    "                        class_weight={0:1, 1:flare_weight}, \n",
    "                        validation_data=generator_val,\n",
    "                        callbacks=[tensorboard_callback, \n",
    "                                   earlystop_callback,\n",
    "                                   checkpoint_callback                               \n",
    "                                  ])\n",
    "\n",
    "    print('Done.')\n",
    "\n",
    "    model.save('LSTM-fold_all-mixedtrain'+str(kfold_ni)+'.h5')\n",
    "    print('Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
